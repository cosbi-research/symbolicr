---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

<!-- TOC start (generated with https://github.com/derlin/bitdowntoc) -->
# Table of Contents

- [SymbolicR](#symbolicr)
   * [Installation](#installation)
   * [Genetic search](#genetic-search)
   * [Random search](#random-search)
   * [Combinatorial Search](#combinatorial-search)
   * [Analyze formula variables](#analyze-formula-variables)
   
<!-- TOC end -->


# SymbolicR

[![DOI](https://zenodo.org/badge/830556463.svg)](https://zenodo.org/doi/10.5281/zenodo.12904321)
<!-- badges: start -->
<!-- badges: end -->

Find non-linear formulas that fits your input data. 
You can systematically explore and memoize the possible formulas and its cross-validation performance, in an incremental fashon. 
Three main interoperable search functions are available: 

- `random.search` performs a random exploration, 
- `genetic.search` employs a genetic optimization algorithm
- `comb.search` allows the user to provide a data.frame of formulas to be tested

After installation, see tutorials with

```r
browseVignettes('symbolicr')
```

## Installation

You can install the development version of symbolicr like so:

``` r
devtools::install_github('cosbi-research/symbolicr', build_vignettes = TRUE)
```

## Genetic search

This is a minimum viable example on how to use genetic search to find the non-linear relationship:

```{r example}
library(symbolicr)
set.seed(1)

x1<-runif(100, min=2, max=67)
x2<-runif(100, min=0.01, max=0.1)

y <- log10(x1^2*x2) + rnorm(100, 0, 0.001)

X <- data.frame(x1=x1, x2=x2)

results <- genetic.search(
  X, y, 
  n.squares=2, 
  max.formula.len = 1, 
  N=2,
  K=10,
  best.vars.l = list(
   c('log.x1')
  ),
  transformations = list(
   "log"=function(rdf, x, stats){ log(x) },
   "inv"=function(rdf, x, stats){ 1/x }
  ),
  keepBest=T,
  cv.norm=F
)

```

We found the correct non-linear formula starting from an initial guess!
We can now get the best formula

```{r best}
results$best
```
And all the formula the genetic algorithm found to be best at each one of the 100 evolution iterations
(last 5 shown for brevity)

```{r evolution}
results$best.iter[seq(length(results$best.iter)-5,length(results$best.iter))]
```
Note that `cv.norm=FALSE` means data is used as-is.
Before running this example we checked for the non-negativeness of `x1^2*x2`.
If you would like to normalize data to avoid scaling issues just use `cv.norm=TRUE` 
but in this case, to avoid computing the log of a negative value, we use this updated transformation function

```{r log-std}
# NB: function applied to standardized X values!
   #     they can be negative
log.std <- function(x, stats){ log(0.1 + abs(stats$min) + x) }
```

The `stats` object contains 
- min: the minimum of the column values
- absmin: the minimum of the absolute values of the columns
- absmax: the maximum of the absolute values of the columns
- projzero: -mean/sd of the columns, that is the position of the zero in the original, non-normalized space.

Type `?dataset.min.maxs` in your R console for further informations.

## Random search

This is a minimum viable example on how to use random search to test multiple non-linear relationships, and get a summary of the performances
in a data.frame:

```{r random.example}

random.results <- random.search(
  X, y, 
  n.squares=2, 
  formula.len = 1, 
  N=2,
  K=10,
  transformations = list(
   "log"=function(rdf, x, stats){ log(x) },
   "inv"=function(rdf, x, stats){ 1/x }
  ),
  cv.norm=F
)

```

You can then inspect results in the resulting data.frame:

```{r random.inspect}
random.results[order(random.results$base.r.squared, decreasing = T), ][seq(5), ]
```

## Combinatorial Search

Search over all non-duplicated combinations of terms taken from an user-supplied list of formulas.


```{r combination.example}

# test the three formula x1, x2 and log(x2)
comb.search(
  X, y, 
  combinations=data.frame(t1=c('x1','x2','log.x2')), 
  N=2,
  K=10,
  transformations = list(
   "log"=function(rdf, x, stats){ log(x) },
   "exp"=function(rdf, x, stats){ exp(x) }
  ),
  cv.norm=F
)

```

## Analyze formula variables

This code highlights the most-important variables among a list of formulas applied to a given dataset.

```{r analyze.variables.example}
max.formula.len=1
transformations=list(
   "log"=function(rdf, x, stats){ log(x) },
   "log_x1_p"=function(rdf, x, stats){ log(rdf$x1 + x) },
   "inv"=function(rdf, x, stats){ 1/x }
)
random.results <- random.search(
  X, y, 
  n.squares=2, 
  formula.len = max.formula.len, 
  N=2,
  K=10,
  transformations = transformations,
  cv.norm=F
)

# compute a unique objective function
random.results$obj <- apply(random.results, MARGIN=1, FUN=function(row) pe.r.squared.formula.len.fitness(as.data.frame(t(row)), max.formula.len))

# sort by top-N functions (according to obj)
ordered.res <- random.results[order(random.results$obj,decreasing=T),]

# max fitness on all computed formulas
best.obj <- ordered.res[1,'obj']
# analyze top-10 formulas
# select formulas according to criterion above
eligible.res <- ordered.res[seq(10), ]

direction = 'max' # obj should be maximized
sensitivity <- analyze.variables(
 X, y, eligible.res, fitness.column='obj',
 # a list of available term transformations
 transformations=transformations,
 # a list of rules to remove a variable from a term
 # ex.
 #   orig transformation -> base transformation, removed term
 #   "log_empty_well_p"=c("log10", "empty_well")
 transformations_replacement_map=list(
   "log_x1_p"=c("log", "x1")
 ),
 custom.abs.mins=list(),
 K=10,
 N=2,
 direction=direction, 
 max.formula.len=max.formula.len,
 fitness.fun=pe.r.squared.formula.len.fitness,
 cv.norm=F
)

# plottable data.frame of quantile losses per-variable
variable.importance.df <- sensitivity[['var.imp']]
n.formulas <- 10
#### PLOT ANALYSIS ####
library(ggplot2)
library(RColorBrewer)
library(colorspace)

colours <- brewer.pal(11, "Paired")
cols_d4 <- darken(colours, 0.4)

#png(file.path('sensitivity', paste("sensitivity.",(percent.limit*100),"percent",type,".png", sep = "")), width = 640, height = 490)
p <- ggplot(variable.importance.df, aes(x=mean.occurrences, y=mean.loss.p, colour=variable)) +
  theme(
    text = element_text(size = 15)
  ) +
  labs(x = paste0("Mean number of occurrences for best ",n.formulas," formulas"), y = "relative loss %") +
  #  geom_crossbar(aes(ymin = lower.loss.p, ymax = higher.loss.p), width = 1.5) +
  geom_pointrange(aes(ymin = lower.loss.p, ymax = higher.loss.p)) +
  scale_y_continuous(trans = 'log10', limits=c(1,110), n.breaks=8,
                     labels =c("1%","3%","5%", "10%", "20%", "30%", "50%", "100%"),
                     breaks =c(1, 3, 5, 10, 20, 30, 50, 100))+
  scale_x_continuous(limits=c(0,3.05), n.breaks=6,
                     labels =c("1/5", "1/2", "3/4", "1", "2", "3"),
                     breaks =c(1/5.0, 1/2.0, 3/4.0, 1, 2, 3))+
  scale_color_manual(values = cols_d4)

print(p)

```

This plot is showing clearly that variable x1 is much more important than variable x2 as it both 

- occours more often in the top-10 formulas, (value on the X axis)
- degrades the performance more when removed from formulas (value on the Y axis)

