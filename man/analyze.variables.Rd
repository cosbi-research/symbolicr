% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/analyze.variables.R
\name{analyze.variables}
\alias{analyze.variables}
\title{Analyze Variables
Get information on the relative importance of each variable included in your formula.}
\usage{
analyze.variables(
  regressors.df,
  y,
  test.formula.df,
  fitness.column,
  transformations,
  transformations_replacement_map,
  custom.abs.mins = list(),
  K = 7,
  N = 10,
  direction = "max",
  max.formula.len = max.formula.len,
  fitness.fun = pe.r.squared.formula.len.fitness,
  cv.norm = T
)
}
\arguments{
\item{regressors.df}{The dataset that contains the base variables the formula is composed of (column-wise)}

\item{y}{The independent variable}

\item{test.formula.df}{A dataset of formulas (contained in 'vars' column) and their fitness values (error measures)}

\item{fitness.column}{The test.formula.df column containing the target fitness value to be used for analysis}

\item{transformations}{A list of potentially non-linear transformations that can be applied on top of the squares. Ex. \verb{order 0, transformation=log10 = log10.a}. Input values are x and z, the array of numbers to be transformed (training set only), and the min max statistics (on the global dataset) respectively.}

\item{transformations_replacement_map}{A list of rules to remove a variable from a term. Ex. \code{"log_empty_well_p"=c("log10", "empty_well")}}

\item{custom.abs.mins}{A list of user-defined minimum values for dataset columns.}

\item{K}{The number of parts the dataset is split into for K-fold cross-validation.}

\item{N}{The number of times the K-fold validation is repeated, shuffling the dataset row orders before each time.}

\item{direction}{'max' if the higher the objective in fitness.column the better, 'min' on the contrary.}

\item{max.formula.len}{The maximum number of terms in the formula}

\item{fitness.fun}{The function that determine the fitness of a given formula. Defaults to \code{pe.r.squared.formula.len.fitness}}

\item{cv.norm}{Normalize regressors after train-validation split in inner cross-validation loop.}
}
\value{
A list of "terms that"loss" and "gain" variables, with a measure of the (relative) gain/loss of removing each variable from the initial formula.
}
\description{
This works by removing one variable at a time from the formula and measuring its new (cross-validation) fitness.
}
\examples{
\dontrun{
# candidates found by random.sarch or genetic.search
# (stored as rData in glob.filepath/local.filepath)
# assumed to be loaded here as 'res'
#
# compute objective function from error measures
res$obj <- apply(res, MARGIN=1, FUN=function(row) pe.r.squared.formula.len.fitness(as.data.frame(t(row)), max.formula.len))
# sort by top-N functions (according to obj)
ordered.res <- res[order(res$obj,decreasing=T),]

# max fitness on all computed formulas
best.obj <- ordered.res[1,'obj']
# take formulas up to 40\% worse than the max
percent.limit <- 0.6
abs.limit <- best.obj*(1-percent.limit)
# select formulas according to criterion above
eligible.res <- ordered.res[ordered.res$obj >= abs.limit, ]

direction = 'max' # obj should be minimized
sensitivity <- analyze.variables(
  data.df, eligible.res, 'obj',
  # a list of available term transformations
  full.transformations,
  # a list of rules to remove a variable from a term
  # ex.
  #   orig transformation -> base transformation, removed term
  #   "log_empty_well_p"=c("log10", "empty_well")
  transformations_replacement_map,
  direction, max.formula.len=4,
  fitness.fun=pe.r.squared.formula.len.fitness
)

# plottable data.frame of quantile losses per-variable
sensitivity[['var.imp']]
# variables that make the formula worse if removed
sensitivity[['formula.loss']]
# variables that improve the formula if removed
sensitivity[['formula.gain']]
}
}
